{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQP0BDzJ97MF"
      },
      "source": [
        "###Name: Bhavesh Bohra (MML2022013)\n",
        "\n",
        "This is your assignment for the 5-7 PM session today. Submission deadline is Sunday 26 Mar; 11:59 PM\n",
        "\n",
        "Perform the following in your lab session.\n",
        "GO to the following Github repository: https://github.com/practical-nlp/practical-nlp-code/tree/master/Ch3. There are 10 notebooks performing various tasks.\n",
        "Create a single Colab notebook tutorial with all the code (from 10 notebooks) and relevant explanation of the code.\n",
        "Run this colab file and explore the implementation of word embeddings.\n",
        "You might need the Google News vectors embeddings available at: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM\n",
        "Modify relevant code to take care of colab/Google drive environment and file path.\n",
        "Using Word2vec, prepare word embedding for all the words from webpage: https://medium.datadriveninvestor.com/convolutional-neural-networks-explained-7fafea4de9c9\n",
        "Use  \"Beautiful Soup\" to extract the data.\n",
        "Perform all preprocessing steps on this text (Tokenization, stemming, lemmatization)\n",
        "As per your word2vec model, find out the most similar words for every word of this article.\n",
        "Check out what happens for OOV problem.  Encapsulate the relevant statements in try/except blocks to make sure your code does not throw any exception.\n",
        "Submit one colab file as your final submission."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41WuIHZk-ET2"
      },
      "source": [
        "###Assignment 5: Word embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nt7FEM-1tmQf"
      },
      "source": [
        "#1) One Hot Encoding of text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQlvhh6kt9ZC",
        "outputId": "9367e1e5-c859-4cbf-c25e-a1e250244524"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scikit-learn==0.21.3\n",
            "  Downloading scikit-learn-0.21.3.tar.gz (12.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn==0.21.3) (1.22.4)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn==0.21.3) (1.10.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.9/dist-packages (from scikit-learn==0.21.3) (1.1.1)\n",
            "Building wheels for collected packages: scikit-learn\n",
            "  Building wheel for scikit-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-learn: filename=scikit_learn-0.21.3-cp39-cp39-linux_x86_64.whl size=23254350 sha256=abe36c232d6d61f31b3b48d798dad7586b68014ee4245744adefbf2a7f46fcb6\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/af/b1/a8d3e09cdde3e64cf498ea5590a915743bd38bd38cd37b0c1e\n",
            "Successfully built scikit-learn\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.5 requires scikit-learn>=1.0.0, but you have scikit-learn 0.21.3 which is incompatible.\n",
            "sklearn-pandas 2.2.0 requires scikit-learn>=0.23.0, but you have scikit-learn 0.21.3 which is incompatible.\n",
            "imbalanced-learn 0.10.1 requires scikit-learn>=1.0.2, but you have scikit-learn 0.21.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed scikit-learn-0.21.3\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn==0.21.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DQ6sHcmuVy3",
        "outputId": "14164d4a-c0e8-47e4-812c-865fe310dfaf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"]\n",
        "processed_docs = [doc.lower().replace(\".\",\"\") for doc in documents]\n",
        "processed_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmIablA-X80i"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xowU1KBLuhIC",
        "outputId": "3eacf311-edbd-44f2-a67e-7cd2d64332e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'dog': 1, 'bites': 2, 'man': 3, 'eats': 4, 'meat': 5, 'food': 6}\n"
          ]
        }
      ],
      "source": [
        "#Build the vocabulary\n",
        "vocab = {}\n",
        "count = 0\n",
        "for doc in processed_docs:\n",
        "    for word in doc.split():\n",
        "        if word not in vocab:\n",
        "            count = count +1\n",
        "            vocab[word] = count\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVFx24GAuka4"
      },
      "outputs": [],
      "source": [
        "#Get one hot representation for any string based on this vocabulary.\n",
        "#If the word exists in the vocabulary, its representation is returned.\n",
        "#If not, a list of zeroes is returned for that word.\n",
        "def get_onehot_vector(somestring):\n",
        "    onehot_encoded = []\n",
        "    for word in somestring.split():\n",
        "        temp = [0]*len(vocab)\n",
        "        if word in vocab:\n",
        "            temp[vocab[word]-1] = 1 # -1 is to take care of the fact indexing in array starts from 0 and not 1\n",
        "        onehot_encoded.append(temp)\n",
        "    return onehot_encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eX4-xMeuma-"
      },
      "outputs": [],
      "source": [
        "print(processed_docs[1])\n",
        "get_onehot_vector(processed_docs[1]) #one hot representation for a text from our corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Q1rf3Deumoy",
        "outputId": "8dd96b9a-ca7c-4331-f7d9-7a81669a8a0a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[0, 0, 1, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0],\n",
              " [1, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0]]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_onehot_vector(\"man and dog are good\")\n",
        "#one hot representation for a random text, using the above vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sch4SKHquqtf",
        "outputId": "0f599643-fe58-474c-a149-cc344f8555b7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[0, 0, 1, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 1, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0]]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_onehot_vector(\"man and man are good\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkQ-sqZou22e"
      },
      "source": [
        "##One-hot encoding using scikit -learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFtm_KPvu5yr"
      },
      "outputs": [],
      "source": [
        "S1 = 'dog bites man'\n",
        "S2 = 'man bites dog'\n",
        "S3 = 'dog eats meat'\n",
        "S4 = 'man eats food'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRi5eQqNu9kl",
        "outputId": "c69519d2-6566-4f6c-b962-5fc5a74621ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The data:  ['dog', 'bites', 'man', 'man', 'bites', 'dog', 'dog', 'eats', 'meat', 'man', 'eats', 'food']\n",
            "Label Encoded: [1 0 4 4 0 1 1 2 5 4 2 3]\n",
            "Onehot Encoded Matrix:\n",
            " [[1. 0. 1. 0. 0. 0. 1. 0.]\n",
            " [0. 1. 1. 0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 1. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 1. 0. 1. 0. 0.]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/utils/multiclass.py:13: DeprecationWarning: Please use `spmatrix` from the `scipy.sparse` namespace, the `scipy.sparse.base` namespace is deprecated.\n",
            "  from scipy.sparse.base import spmatrix\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_encoders.py:395: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  check_array(X, dtype=np.int)\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_encoders.py:52: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  X = check_array(X, dtype=np.object)\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_encoders.py:52: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  X = check_array(X, dtype=np.object)\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_encoders.py:110: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  X_int = np.zeros((n_samples, n_features), dtype=np.int)\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_encoders.py:111: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  X_mask = np.ones((n_samples, n_features), dtype=np.bool)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "data = [S1.split(), S2.split(), S3.split(), S4.split()]\n",
        "values = data[0]+data[1]+data[2]+data[3]\n",
        "print(\"The data: \",values)\n",
        "\n",
        "#Label Encoding\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(values)\n",
        "print(\"Label Encoded:\",integer_encoded)\n",
        "\n",
        "#One-Hot Encoding\n",
        "onehot_encoder = OneHotEncoder()\n",
        "onehot_encoded = onehot_encoder.fit_transform(data).toarray()\n",
        "print(\"Onehot Encoded Matrix:\\n\",onehot_encoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wP-hZi5PvfrI"
      },
      "source": [
        "#2) Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tc1r4_NNvp4b",
        "outputId": "72df58fb-5141-4c47-8d23-a1c57f9991e1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"] #Same as the earlier notebook\n",
        "processed_docs = [doc.lower().replace(\".\",\"\") for doc in documents]\n",
        "processed_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_z934r9vsc-",
        "outputId": "51ac8628-5ac8-4318-bbf7-30ea4a7229c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Our corpus:  ['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']\n",
            "Our vocabulary:  {'dog': 1, 'bites': 0, 'man': 4, 'eats': 2, 'meat': 5, 'food': 3}\n",
            "BoW representation for 'dog bites man':  [[1 1 0 0 1 0]]\n",
            "BoW representation for 'man bites dog:  [[1 1 0 0 1 0]]\n",
            "Bow representation for 'dog and dog are friends': [[0 2 0 0 0 0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  dtype=np.int):\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#look at the documents list\n",
        "print(\"Our corpus: \", processed_docs)\n",
        "\n",
        "count_vect = CountVectorizer()\n",
        "#Build a BOW representation for the corpus\n",
        "bow_rep = count_vect.fit_transform(processed_docs)\n",
        "\n",
        "#Look at the vocabulary mapping\n",
        "print(\"Our vocabulary: \", count_vect.vocabulary_)\n",
        "\n",
        "#see the BOW rep for first 2 documents\n",
        "print(\"BoW representation for 'dog bites man': \", bow_rep[0].toarray())\n",
        "print(\"BoW representation for 'man bites dog: \",bow_rep[1].toarray())\n",
        "\n",
        "#Get the representation using this vocabulary, for a new text\n",
        "temp = count_vect.transform([\"dog and dog are friends\"])\n",
        "print(\"Bow representation for 'dog and dog are friends':\", temp.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJG1dshSvwry",
        "outputId": "465220ef-515e-4a6c-99c2-869f8a437f7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bow representation for 'dog and dog are friends': [[0 1 0 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "#BoW with binary vectors\n",
        "count_vect = CountVectorizer(binary=True)\n",
        "count_vect.fit(processed_docs)\n",
        "temp = count_vect.transform([\"dog and dog are friends\"])\n",
        "print(\"Bow representation for 'dog and dog are friends':\", temp.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7vmHYgev71x"
      },
      "source": [
        "#3) Bag of N-Grams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfaDJsEbwIQt",
        "outputId": "c3c6143a-712a-4497-8c87-49c5893ee3f4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#our corpus\n",
        "documents = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"]\n",
        "\n",
        "processed_docs = [doc.lower().replace(\".\",\"\") for doc in documents]\n",
        "processed_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ykTGBSLwOKi",
        "outputId": "abd65d3c-2ea4-4da8-af8c-6799aa34bf52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Our vocabulary:  {'dog': 3, 'bites': 0, 'man': 12, 'dog bites': 4, 'bites man': 2, 'dog bites man': 5, 'man bites': 13, 'bites dog': 1, 'man bites dog': 14, 'eats': 8, 'meat': 17, 'dog eats': 6, 'eats meat': 10, 'dog eats meat': 7, 'food': 11, 'man eats': 15, 'eats food': 9, 'man eats food': 16}\n",
            "BoW representation for 'dog bites man':  [[1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0]]\n",
            "BoW representation for 'man bites dog:  [[1 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0]]\n",
            "Bow representation for 'dog and dog are friends': [[0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#Ngram vectorization example with count vectorizer and uni, bi, trigrams\n",
        "count_vect = CountVectorizer(ngram_range=(1,3))\n",
        "\n",
        "#Build a BOW representation for the corpus\n",
        "bow_rep = count_vect.fit_transform(processed_docs)\n",
        "\n",
        "#Look at the vocabulary mapping\n",
        "print(\"Our vocabulary: \", count_vect.vocabulary_)\n",
        "\n",
        "#see the BOW rep for first 2 documents\n",
        "print(\"BoW representation for 'dog bites man': \", bow_rep[0].toarray())\n",
        "print(\"BoW representation for 'man bites dog: \",bow_rep[1].toarray())\n",
        "\n",
        "#Get the representation using this vocabulary, for a new text\n",
        "temp = count_vect.transform([\"dog and dog are friends\"])\n",
        "\n",
        "print(\"Bow representation for 'dog and dog are friends':\", temp.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USTkUY5iwVv5"
      },
      "source": [
        "#4) TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4bsSgv_wbqv",
        "outputId": "bc0d23f4-ce2e-481c-cb8b-da6de1796397"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"]\n",
        "processed_docs = [doc.lower().replace(\".\",\"\") for doc in documents]\n",
        "processed_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s57RdltQwgey",
        "outputId": "8c3ec923-5761-40c0-b96f-688351e50172"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IDF for all words in the vocabulary [1.51082562 1.22314355 1.51082562 1.91629073 1.22314355 1.91629073]\n",
            "----------\n",
            "All words in the vocabulary ['bites', 'dog', 'eats', 'food', 'man', 'meat']\n",
            "----------\n",
            "TFIDF representation for all documents in our corpus\n",
            " [[0.65782931 0.53256952 0.         0.         0.53256952 0.        ]\n",
            " [0.65782931 0.53256952 0.         0.         0.53256952 0.        ]\n",
            " [0.         0.44809973 0.55349232 0.         0.         0.70203482]\n",
            " [0.         0.         0.55349232 0.70203482 0.44809973 0.        ]]\n",
            "----------\n",
            "Tfidf representation for 'dog and man are friends':\n",
            " [[0.         0.70710678 0.         0.         0.70710678 0.        ]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "bow_rep_tfidf = tfidf.fit_transform(processed_docs)\n",
        "\n",
        "#IDF for all words in the vocabulary\n",
        "print(\"IDF for all words in the vocabulary\",tfidf.idf_)\n",
        "print(\"-\"*10)\n",
        "#All words in the vocabulary.\n",
        "print(\"All words in the vocabulary\",tfidf.get_feature_names())\n",
        "print(\"-\"*10)\n",
        "\n",
        "#TFIDF representation for all documents in our corpus\n",
        "print(\"TFIDF representation for all documents in our corpus\\n\",bow_rep_tfidf.toarray())\n",
        "print(\"-\"*10)\n",
        "\n",
        "temp = tfidf.transform([\"dog and man are friends\"])\n",
        "print(\"Tfidf representation for 'dog and man are friends':\\n\", temp.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gwln8mWuxKeP"
      },
      "source": [
        "#5) Pre-trained word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xmewAZ526Xr"
      },
      "source": [
        "##1. Using a pre-trained word2vec model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "joO4ociXxiEZ",
        "outputId": "0806efbe-4ad6-4d8b-b279-ffeeeb6636e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn==0.21.3 in /usr/local/lib/python3.9/dist-packages (0.21.3)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn==0.21.3) (1.22.4)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn==0.21.3) (1.10.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.9/dist-packages (from scikit-learn==0.21.3) (1.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wget==3.2\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9676 sha256=21ea300b74f471bbcc9499882ea0ff31b776019806e99af82813e075ba5bc619\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/5f/3e/46cc37c5d698415694d83f607f833f83f0149e49b3af9d0f38\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gensim==3.6.0\n",
            "  Downloading gensim-3.6.0.tar.gz (23.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.1/23.1 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.9/dist-packages (from gensim==3.6.0) (1.22.4)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.9/dist-packages (from gensim==3.6.0) (1.10.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.9/dist-packages (from gensim==3.6.0) (1.16.0)\n",
            "Requirement already satisfied: smart_open>=1.2.1 in /usr/local/lib/python3.9/dist-packages (from gensim==3.6.0) (6.3.0)\n",
            "Building wheels for collected packages: gensim\n",
            "  Building wheel for gensim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gensim: filename=gensim-3.6.0-cp39-cp39-linux_x86_64.whl size=23915463 sha256=d32574bc69b54121d75b87f05eff1b84314c12cf587d60358f2e1f5d3ebe821d\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/12/f2/84de20fba5e870553796b0834d11109992f06ddc20aaead086\n",
            "Successfully built gensim\n",
            "Installing collected packages: gensim\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 4.3.1\n",
            "    Uninstalling gensim-4.3.1:\n",
            "      Successfully uninstalled gensim-4.3.1\n",
            "Successfully installed gensim-3.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting psutil==5.4.8\n",
            "  Downloading psutil-5.4.8.tar.gz (422 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.7/422.7 KB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: psutil\n",
            "  Building wheel for psutil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for psutil: filename=psutil-5.4.8-cp39-cp39-linux_x86_64.whl size=275975 sha256=5a5d48295cf505da54b0ab3ed2cbc4df029c6f50bf256ee19ae5f775ba2eda08\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/76/c4/5d31c7c3bfad0582bbed0d8bbad61faf11461127fd147f77a0\n",
            "Successfully built psutil\n",
            "Installing collected packages: psutil\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.9.4\n",
            "    Uninstalling psutil-5.9.4:\n",
            "      Successfully uninstalled psutil-5.9.4\n",
            "Successfully installed psutil-5.4.8\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "psutil"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spacy==2.2.4\n",
            "  Downloading spacy-2.2.4.tar.gz (6.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy==2.2.4) (1.0.9)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy==2.2.4) (4.65.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy==2.2.4) (2.0.7)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy==2.2.4) (1.22.4)\n",
            "Collecting thinc==7.4.0\n",
            "  Using cached thinc-7.4.0-cp39-cp39-linux_x86_64.whl\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy==2.2.4) (3.0.8)\n",
            "Collecting srsly<1.1.0,>=1.0.2\n",
            "  Using cached srsly-1.0.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209 kB)\n",
            "Collecting plac<1.2.0,>=0.9.6\n",
            "  Using cached plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy==2.2.4) (2.27.1)\n",
            "Collecting blis<0.5.0,>=0.4.0\n",
            "  Using cached blis-0.4.1-cp39-cp39-linux_x86_64.whl\n",
            "Collecting catalogue<1.1.0,>=0.0.7\n",
            "  Using cached catalogue-1.0.2-py2.py3-none-any.whl (16 kB)\n",
            "Collecting wasabi<1.1.0,>=0.4.0\n",
            "  Using cached wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy==2.2.4) (67.6.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4) (2.0.12)\n",
            "Building wheels for collected packages: spacy\n",
            "  Building wheel for spacy (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spacy: filename=spacy-2.2.4-cp39-cp39-linux_x86_64.whl size=29560214 sha256=6aea0bbd17df662a83f59cd4e87625952b74d2f7393f6f9a2346a9b8d6e3aeb4\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/07/92/5f1eb398c6e7124a9cf7af79aa2c0d44a307bb229d1b361428\n",
            "Successfully built spacy\n",
            "Installing collected packages: wasabi, plac, srsly, catalogue, blis, thinc, spacy\n",
            "  Attempting uninstall: wasabi\n",
            "    Found existing installation: wasabi 1.1.1\n",
            "    Uninstalling wasabi-1.1.1:\n",
            "      Successfully uninstalled wasabi-1.1.1\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 2.4.6\n",
            "    Uninstalling srsly-2.4.6:\n",
            "      Successfully uninstalled srsly-2.4.6\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 2.0.8\n",
            "    Uninstalling catalogue-2.0.8:\n",
            "      Successfully uninstalled catalogue-2.0.8\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 0.7.9\n",
            "    Uninstalling blis-0.7.9:\n",
            "      Successfully uninstalled blis-0.7.9\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.9\n",
            "    Uninstalling thinc-8.1.9:\n",
            "      Successfully uninstalled thinc-8.1.9\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.5.1\n",
            "    Uninstalling spacy-3.5.1:\n",
            "      Successfully uninstalled spacy-3.5.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.5.0 requires spacy<3.6.0,>=3.5.0, but you have spacy 2.2.4 which is incompatible.\n",
            "confection 0.0.4 requires srsly<3.0.0,>=2.4.0, but you have srsly 1.0.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed blis-0.4.1 catalogue-1.0.2 plac-1.1.3 spacy-2.2.4 srsly-1.0.6 thinc-7.4.0 wasabi-0.10.1\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn==0.21.3\n",
        "!pip install wget==3.2\n",
        "!pip install gensim==3.6.0\n",
        "!pip install psutil==5.4.8\n",
        "!pip install spacy==2.2.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDdlUnLNxPZx",
        "outputId": "5c2d7eec-6704-41d0-9e8e-50dc5f75e33c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model at /content/drive/MyDrive/NLP/GoogleNews-vectors-negative300.bin\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import wget\n",
        "import gzip\n",
        "import shutil\n",
        "\n",
        "gn_vec_path = \"/content/drive/MyDrive/NLP/GoogleNews-vectors-negative300.bin\"\n",
        "\n",
        "'''if not os.path.exists(\"GoogleNews-vectors-negative300.bin\"):\n",
        "    if not os.path.exists(\"../Ch2/GoogleNews-vectors-negative300.bin\"):\n",
        "        #Downloading the reqired model\n",
        "        if not os.path.exists(\"../Ch2/GoogleNews-vectors-negative300.bin.gz\"):\n",
        "            if not os.path.exists(\"GoogleNews-vectors-negative300.bin.gz\"):\n",
        "                wget.download(\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\")\n",
        "            gn_vec_zip_path = \"GoogleNews-vectors-negative300.bin.gz\"\n",
        "        else:\n",
        "            gn_vec_zip_path = \"../Ch2/GoogleNews-vectors-negative300.bin.gz\"\n",
        "        #Extracting the required model\n",
        "        with gzip.open(gn_vec_zip_path, 'rb') as f_in:\n",
        "            with open(gn_vec_path, 'wb') as f_out:\n",
        "                shutil.copyfileobj(f_in, f_out)\n",
        "    else:\n",
        "        gn_vec_path = \"/content/sample_data\" + gn_vec_path\n",
        "'''\n",
        "print(f\"Model at {gn_vec_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39_JQeGCzEQY"
      },
      "outputs": [],
      "source": [
        "import warnings #This module ignores the various types of warnings generated\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import psutil #This module helps in retrieving information on running processes and system resource utilization\n",
        "process = psutil.Process(os.getpid())\n",
        "from psutil import virtual_memory\n",
        "mem = virtual_memory()\n",
        "\n",
        "import time #This module is used to calculate the time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "oYMD-_j1zLUU",
        "outputId": "e4f409c6-a46a-4d5a-b37a-345392e7dc99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Memory used in GB before Loading the Model: 0.17\n",
            "----------\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-7a2bcb30997e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mttl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;31m#Toal memory available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mw2v_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrainedpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#load the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%0.2f seconds taken to load\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Calculate the total time elapsed since starting the timer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1434\u001b[0m         \"\"\"\n\u001b[1;32m   1435\u001b[0m         \u001b[0;31m# from gensim.models.word2vec import load_word2vec_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1436\u001b[0;31m         return _load_word2vec_format(\n\u001b[0m\u001b[1;32m   1437\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1438\u001b[0m             limit=limit, datatype=datatype)\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# throws for invalid file format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mso_compression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFER_FROM_EXTENSION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mtransport_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     fobj = _shortcut_open(\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/NLP/GoogleNews-vectors-negative300.bin'"
          ]
        }
      ],
      "source": [
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "pretrainedpath = gn_vec_path\n",
        "\n",
        "#Load W2V model. This will take some time, but it is a one time effort!\n",
        "pre = process.memory_info().rss\n",
        "print(\"Memory used in GB before Loading the Model: %0.2f\"%float(pre/(10**9))) #Check memory usage before loading the model\n",
        "print('-'*10)\n",
        "\n",
        "start_time = time.time() #Start the timer\n",
        "ttl = mem.total #Toal memory available\n",
        "\n",
        "w2v_model = KeyedVectors.load_word2vec_format(pretrainedpath, binary=True) #load the model\n",
        "print(\"%0.2f seconds taken to load\"%float(time.time() - start_time)) #Calculate the total time elapsed since starting the timer\n",
        "print('-'*10)\n",
        "\n",
        "print('Finished loading Word2Vec')\n",
        "print('-'*10)\n",
        "\n",
        "post = process.memory_info().rss\n",
        "print(\"Memory used in GB after Loading the Model: {:.2f}\".format(float(post/(10**9)))) #Calculate the memory used after loading the model\n",
        "print('-'*10)\n",
        "\n",
        "print(\"Percentage increase in memory usage: {:.2f}% \".format(float((post/pre)*100))) #Percentage increase in memory after loading the model\n",
        "print('-'*10)\n",
        "\n",
        "print(\"Numver of words in vocablulary: \",len(w2v_model.vocab)) #Number of words in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "h_WxKOXDzOnf",
        "outputId": "bfc4fce4-46d4-4e95-f421-72778109f41e"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-78a2b1955943>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Let us examine the model by knowing what the most similar words are, for a given word!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mw2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'beautiful'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'w2v_model' is not defined"
          ]
        }
      ],
      "source": [
        "#Let us examine the model by knowing what the most similar words are, for a given word!\n",
        "w2v_model.most_similar('beautiful')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ce2CToG1zQ0y"
      },
      "outputs": [],
      "source": [
        "#Let us try with another word!\n",
        "w2v_model.most_similar('toronto')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FrmsLk7zT7m"
      },
      "outputs": [],
      "source": [
        "#What is the vector representation for a word?\n",
        "w2v_model['computer']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsDkvbgVze9s"
      },
      "source": [
        "##2. Getting the embedding representation for full text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SJOO9nZzhqD",
        "outputId": "7bd18903-c441-4765-8afb-b5240f9f3e5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en_core_web_md==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz (96.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 MB\u001b[0m \u001b[31m175.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.9/dist-packages (from en_core_web_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.7)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.9/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.9)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.27.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (67.6.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.65.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.9/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2022.12.7)\n",
            "Building wheels for collected packages: en_core_web_md\n",
            "  Building wheel for en_core_web_md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en_core_web_md: filename=en_core_web_md-2.2.5-py3-none-any.whl size=98051300 sha256=2ec8b5bed9e59d3279a4e089fcf118d946d706f5f97e7644ed95f2758c6216c3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-hq0zcfkb/wheels/f1/90/58/4b7b9872d3f397191467333e20f21f073e12d91da2e76d6211\n",
            "Successfully built en_core_web_md\n",
            "Installing collected packages: en_core_web_md\n",
            "Successfully installed en_core_web_md-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "M_czdg48zlLT",
        "outputId": "7981bd81-b74a-4f19-8578-fc8f4ba8dcc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
            "Wall time: 5.01 µs\n"
          ]
        },
        {
          "ename": "OSError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-2bd56081e19d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_md'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# process a sentence using the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmydoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Canada is a large country\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_md'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "%time\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "# process a sentence using the model\n",
        "mydoc = nlp(\"Canada is a large country\")\n",
        "#Get a vector for individual words\n",
        "#print(doc[0].vector) #vector for 'Canada', the first word in the text\n",
        "print(mydoc.vector) #Averaged vector for the entire sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3AcddAAzoBD"
      },
      "outputs": [],
      "source": [
        "#What happens when I give a sentence with strange words (and stop words), and try to get its word vector in Spacy?\n",
        "temp = nlp('practicalnlp is a newword')\n",
        "temp[0].vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKRJnC87NYhG"
      },
      "source": [
        "#6) Training Embeddings Using Gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHznA_VlNqaf",
        "outputId": "c93de6cf-50f0-4282-d8b1-c71fe735007d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim==3.6.0 in /usr/local/lib/python3.9/dist-packages (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.9/dist-packages (from gensim==3.6.0) (1.22.4)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.9/dist-packages (from gensim==3.6.0) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.9/dist-packages (from gensim==3.6.0) (6.3.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.9/dist-packages (from gensim==3.6.0) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting requests==2.23.0\n",
            "  Downloading requests-2.23.0-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 KB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.0/128.0 KB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests==2.23.0) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/lib/python3/dist-packages (from requests==2.23.0) (3.0.4)\n",
            "Collecting idna<3,>=2.5\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 KB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: urllib3, idna, requests\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.15\n",
            "    Uninstalling urllib3-1.26.15:\n",
            "      Successfully uninstalled urllib3-1.26.15\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.4\n",
            "    Uninstalling idna-3.4:\n",
            "      Successfully uninstalled idna-3.4\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.27.1\n",
            "    Uninstalling requests-2.27.1:\n",
            "      Successfully uninstalled requests-2.27.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yfinance 0.2.13 requires requests>=2.26, but you have requests 2.23.0 which is incompatible.\n",
            "tweepy 4.13.0 requires requests<3,>=2.27.0, but you have requests 2.23.0 which is incompatible.\n",
            "pandas-profiling 3.2.0 requires requests>=2.24.0, but you have requests 2.23.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests>=2.27.0, but you have requests 2.23.0 which is incompatible.\n",
            "en-core-web-sm 3.5.0 requires spacy<3.6.0,>=3.5.0, but you have spacy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed idna-2.10 requests-2.23.0 urllib3-1.25.11\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim==3.6.0\n",
        "!pip install requests==2.23.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWG3W2MUN4se"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncrx6Vx-N82X",
        "outputId": "91ff11ad-c475-4bdd-f2e0-afc7abb0f398"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
            "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
          ]
        }
      ],
      "source": [
        "# define training data\n",
        "#Genism word2vec requires that a format of ‘list of lists’ be provided for training where every document contained in a list.\n",
        "#Every list contains lists of tokens of that document.\n",
        "corpus = [['dog','bites','man'], [\"man\", \"bites\" ,\"dog\"],[\"dog\",\"eats\",\"meat\"],[\"man\", \"eats\",\"food\"]]\n",
        "\n",
        "#Training the model\n",
        "model_cbow = Word2Vec(corpus, min_count=1,sg=0) #using CBOW Architecture for trainnig\n",
        "model_skipgram = Word2Vec(corpus, min_count=1,sg=1)#using skipGram Architecture for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EToKTkN_OFkL"
      },
      "source": [
        "##Continuous Bag of Words (CBOW)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SznJCDKkOCpt",
        "outputId": "67478426-7601-4eec-d59a-5a63dcd03f00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2Vec(vocab=6, size=100, alpha=0.025)\n",
            "['dog', 'bites', 'man', 'eats', 'meat', 'food']\n",
            "[ 4.13013622e-03 -2.25737854e-03 -2.79160659e-03 -1.79776398e-03\n",
            " -1.19473960e-03  4.66820737e-03  2.49017449e-03 -3.40346992e-03\n",
            "  2.72987207e-04  4.08694427e-03 -4.05620784e-03 -4.22854489e-03\n",
            " -4.43256111e-04  3.21406941e-03 -1.90036057e-03  3.81624862e-03\n",
            " -1.42368989e-03 -3.07270931e-03 -2.87174899e-03 -3.49970505e-04\n",
            " -1.93426362e-03  9.25058383e-04  4.55249473e-03 -3.34360427e-03\n",
            "  4.96405223e-03  1.75382290e-03 -4.95757954e-03 -2.00275448e-03\n",
            " -2.42023193e-03 -1.32689730e-03 -2.17056996e-03 -4.35950584e-04\n",
            "  4.10917634e-03 -2.58322386e-03 -9.28040943e-04 -9.76793468e-04\n",
            " -1.16875267e-03 -4.32980666e-03  3.82712623e-03 -4.18289746e-05\n",
            "  3.74106155e-03 -3.36863543e-03 -3.26203811e-03  2.39519868e-03\n",
            " -2.01245025e-03 -4.60126856e-03  4.46850946e-03  4.40509990e-03\n",
            "  2.85990140e-03  2.08796118e-03 -2.61615729e-03 -4.87835566e-03\n",
            "  2.39057164e-03  1.17336254e-04 -4.39656526e-03 -3.53468652e-03\n",
            "  5.13235922e-04 -1.78986011e-04  1.83128170e-03 -3.84576968e-03\n",
            "  4.17314190e-03 -3.72504466e-03 -1.29501580e-03  3.54268868e-03\n",
            "  3.72253964e-03  7.86409422e-04 -1.91058416e-03  4.54259617e-03\n",
            "  2.77019106e-03 -2.54488853e-03  3.45577719e-03  1.05241612e-04\n",
            "  8.19995883e-04  3.38896061e-03 -2.33546773e-04 -1.39263168e-03\n",
            "  1.45831739e-03  4.04860452e-03 -2.59168656e-03  9.09700408e-04\n",
            "  2.82424991e-03 -2.40092562e-03 -4.96300450e-03 -2.07234942e-03\n",
            " -2.09835707e-03  4.43292520e-04  4.21503279e-03  4.03207960e-04\n",
            " -1.73245487e-03 -3.53027531e-03 -4.52175550e-03  1.20035396e-03\n",
            "  3.73136997e-03  4.24473081e-04  2.94786849e-04 -1.86935754e-03\n",
            "  3.51508206e-04 -4.09935135e-03 -1.52187876e-03 -2.66233971e-03]\n"
          ]
        }
      ],
      "source": [
        "#Summarize the loaded model\n",
        "print(model_cbow)\n",
        "\n",
        "#Summarize vocabulary\n",
        "words = list(model_cbow.wv.vocab)\n",
        "print(words)\n",
        "\n",
        "#Acess vector for one word\n",
        "print(model_cbow['dog'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNLIysapONpd",
        "outputId": "940afe1b-ff75-4673-8b6a-19dfaf7b3708"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarity between eats and bites: 0.015838645\n",
            "Similarity between eats and man: -0.07992174\n"
          ]
        }
      ],
      "source": [
        "#Compute similarity\n",
        "print(\"Similarity between eats and bites:\",model_cbow.similarity('eats', 'bites'))\n",
        "print(\"Similarity between eats and man:\",model_cbow.similarity('eats', 'man'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boQG2OfmOSZD",
        "outputId": "6deaa53b-4292-46cb-962f-ea215cd98d4e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('bites', 0.12030280381441116),\n",
              " ('eats', 0.11944545060396194),\n",
              " ('man', 0.04577469453215599),\n",
              " ('food', -0.02975611388683319),\n",
              " ('dog', -0.05546269565820694)]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Most similarity\n",
        "model_cbow.most_similar('meat')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxcrrOPxOVhJ",
        "outputId": "fe6da9fc-3669-4552-912f-56c730b950c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2Vec(vocab=6, size=100, alpha=0.025)\n"
          ]
        }
      ],
      "source": [
        "# save model\n",
        "model_cbow.save('model_cbow.bin')\n",
        "\n",
        "# load model\n",
        "new_model_cbow = Word2Vec.load('model_cbow.bin')\n",
        "print(new_model_cbow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VKJ5fe2OX6I"
      },
      "source": [
        "##SkipGram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cbphkxQOanr",
        "outputId": "dff1a6c4-4364-4728-92ba-4a48238cab45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2Vec(vocab=6, size=100, alpha=0.025)\n",
            "['dog', 'bites', 'man', 'eats', 'meat', 'food']\n",
            "[ 4.13013622e-03 -2.25737854e-03 -2.79160659e-03 -1.79776398e-03\n",
            " -1.19473960e-03  4.66820737e-03  2.49017449e-03 -3.40346992e-03\n",
            "  2.72987207e-04  4.08694427e-03 -4.05620784e-03 -4.22854489e-03\n",
            " -4.43256111e-04  3.21406941e-03 -1.90036057e-03  3.81624862e-03\n",
            " -1.42368989e-03 -3.07270931e-03 -2.87174899e-03 -3.49970505e-04\n",
            " -1.93426362e-03  9.25058383e-04  4.55249473e-03 -3.34360427e-03\n",
            "  4.96405223e-03  1.75382290e-03 -4.95757954e-03 -2.00275448e-03\n",
            " -2.42023193e-03 -1.32689730e-03 -2.17056996e-03 -4.35950584e-04\n",
            "  4.10917634e-03 -2.58322386e-03 -9.28040943e-04 -9.76793468e-04\n",
            " -1.16875267e-03 -4.32980666e-03  3.82712623e-03 -4.18289746e-05\n",
            "  3.74106155e-03 -3.36863543e-03 -3.26203811e-03  2.39519868e-03\n",
            " -2.01245025e-03 -4.60126856e-03  4.46850946e-03  4.40509990e-03\n",
            "  2.85990140e-03  2.08796118e-03 -2.61615729e-03 -4.87835566e-03\n",
            "  2.39057164e-03  1.17336254e-04 -4.39656526e-03 -3.53468652e-03\n",
            "  5.13235922e-04 -1.78986011e-04  1.83128170e-03 -3.84576968e-03\n",
            "  4.17314190e-03 -3.72504466e-03 -1.29501580e-03  3.54268868e-03\n",
            "  3.72253964e-03  7.86409422e-04 -1.91058416e-03  4.54259617e-03\n",
            "  2.77019106e-03 -2.54488853e-03  3.45577719e-03  1.05241612e-04\n",
            "  8.19995883e-04  3.38896061e-03 -2.33546773e-04 -1.39263168e-03\n",
            "  1.45831739e-03  4.04860452e-03 -2.59168656e-03  9.09700408e-04\n",
            "  2.82424991e-03 -2.40092562e-03 -4.96300450e-03 -2.07234942e-03\n",
            " -2.09835707e-03  4.43292520e-04  4.21503279e-03  4.03207960e-04\n",
            " -1.73245487e-03 -3.53027531e-03 -4.52175550e-03  1.20035396e-03\n",
            "  3.73136997e-03  4.24473081e-04  2.94786849e-04 -1.86935754e-03\n",
            "  3.51508206e-04 -4.09935135e-03 -1.52187876e-03 -2.66233971e-03]\n"
          ]
        }
      ],
      "source": [
        "#Summarize the loaded model\n",
        "print(model_skipgram)\n",
        "\n",
        "#Summarize vocabulary\n",
        "words = list(model_skipgram.wv.vocab)\n",
        "print(words)\n",
        "\n",
        "#Acess vector for one word\n",
        "print(model_skipgram['dog'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggLgqgHZOgPd",
        "outputId": "8041a46e-95fb-449e-a92f-f17146d48922"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarity between eats and bites: 0.015829632\n",
            "Similarity between eats and man: -0.079925954\n"
          ]
        }
      ],
      "source": [
        "#Compute similarity\n",
        "print(\"Similarity between eats and bites:\",model_skipgram.similarity('eats', 'bites'))\n",
        "print(\"Similarity between eats and man:\",model_skipgram.similarity('eats', 'man'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUtUFQB9OjMz"
      },
      "outputs": [],
      "source": [
        "#Most similarity\n",
        "model_skipgram.most_similar('meat')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13YRmklTOlqN",
        "outputId": "89a774a0-c36b-46b3-fe59-9122a67980be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2Vec(vocab=6, size=100, alpha=0.025)\n"
          ]
        }
      ],
      "source": [
        "# save model\n",
        "model_skipgram.save('model_skipgram.bin')\n",
        "\n",
        "# load model\n",
        "new_model_skipgram = Word2Vec.load('model_skipgram.bin')\n",
        "print(new_model_skipgram)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXief9fuOo73"
      },
      "source": [
        "##Training Your Embedding on Wiki Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZjaqJk-Ot0E",
        "outputId": "5735bdee-992d-4f28-be51-fd16da5f138d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File at: data/en/enwiki-latest-pages-articles-multistream14.xml-p13159683p14324602.bz2\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "os.makedirs('data/en', exist_ok= True)\n",
        "file_name = \"data/en/enwiki-latest-pages-articles-multistream14.xml-p13159683p14324602.bz2\"\n",
        "file_id = \"11804g0GcWnBIVDahjo5fQyc05nQLXGwF\"\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "    save_response_content(response, destination)\n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "if not os.path.exists(file_name):\n",
        "    download_file_from_google_drive(file_id, file_name)\n",
        "else:\n",
        "    print(\"file already exists, skipping download\")\n",
        "\n",
        "print(f\"File at: {file_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFPRK2ztO1xN"
      },
      "outputs": [],
      "source": [
        "from gensim.corpora.wikicorpus import WikiCorpus\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from gensim.models.fasttext import FastText\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjBD0FR7O4Wb",
        "outputId": "7bc0b925-b440-41ed-de94-540afaef2b3e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Process InputQueue-2:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/gensim/utils.py\", line 1197, in run\n",
            "    wrapped_chunk = [list(chunk)]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/gensim/corpora/wikicorpus.py\", line 667, in <genexpr>\n",
            "    ((text, self.lemmatize, title, pageid, tokenization_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/gensim/corpora/wikicorpus.py\", line 410, in extract_pages\n",
            "    elem = next(elems)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/gensim/corpora/wikicorpus.py\", line 404, in <genexpr>\n",
            "    elems = (elem for _, elem in iterparse(f, events=(\"end\",)))\n",
            "  File \"/usr/lib/python3.9/xml/etree/ElementTree.py\", line 1255, in iterator\n",
            "    data = source.read(16 * 1024)\n",
            "  File \"/usr/lib/python3.9/bz2.py\", line 171, in read\n",
            "    return self._buffer.read(size)\n",
            "  File \"/usr/lib/python3.9/_compression.py\", line 68, in readinto\n",
            "    data = self.read(len(byte_view))\n",
            "  File \"/usr/lib/python3.9/_compression.py\", line 103, in read\n",
            "    data = self._decompressor.decompress(rawblock, size)\n",
            "OSError: Invalid data stream\n"
          ]
        }
      ],
      "source": [
        "#Preparing the Training data\n",
        "#wiki = WikiCorpus(file_name, lemmatize=False, dictionary={})\n",
        "#sentences = list(wiki.get_texts())\n",
        "\n",
        "#if you get a memory error executing the lines above\n",
        "#comment the lines out and uncomment the lines below.\n",
        "#loading will be slower, but stable.\n",
        "wiki = WikiCorpus(file_name, processes=1, lemmatize=False, dictionary={})\n",
        "sentences = list(wiki.get_texts())\n",
        "\n",
        "#if you still get a memory error, try settings processes to 1 or 2 and then run it again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0I2Moh2NPWm7"
      },
      "source": [
        "#7) Document Vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jekNniU5rNQ4"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQp88UQDrVVR"
      },
      "outputs": [],
      "source": [
        "#Import spacy and load the model\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\") #here nlp object refers to the 'en_core_web_sm' language model instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MafUplEyrXrJ"
      },
      "outputs": [],
      "source": [
        "#Assume each sentence in documents corresponds to a separate document.\n",
        "documents = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"]\n",
        "processed_docs = [doc.lower().replace(\".\",\"\") for doc in documents]\n",
        "processed_docs\n",
        "\n",
        "print(\"Document After Pre-Processing:\",processed_docs)\n",
        "\n",
        "\n",
        "#Iterate over each document and initiate an nlp instance.\n",
        "for doc in processed_docs:\n",
        "    doc_nlp = nlp(doc) #creating a spacy \"Doc\" object which is a container for accessing linguistic annotations.\n",
        "\n",
        "    print(\"-\"*30)\n",
        "    print(\"Average Vector of '{}'\\n\".format(doc),doc_nlp.vector)#this gives the average vector of each document\n",
        "    for token in doc_nlp:\n",
        "        print()\n",
        "        print(token.text,token.vector)#this gives the text of each word in the doc and their respective vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3l-sTlsrkDk"
      },
      "source": [
        "#8) Doc2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcreRi-Arrbo"
      },
      "outputs": [],
      "source": [
        "!pip install gensim==3.6.0\n",
        "!pip install spacy==2.2.4\n",
        "!pip install nltk==3.2.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKM-PgSurw9Z"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from nltk.tokenize import word_tokenize\n",
        "from pprint import pprint\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CiMWNd3r4ib"
      },
      "outputs": [],
      "source": [
        "data = [\"dog bites man\",\n",
        "        \"man bites dog\",\n",
        "        \"dog eats meat\",\n",
        "        \"man eats food\"]\n",
        "\n",
        "tagged_data = [TaggedDocument(words=word_tokenize(word.lower()), tags=[str(i)]) for i, word in enumerate(data)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDTkO9wDr6jj"
      },
      "outputs": [],
      "source": [
        "tagged_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-C9uFeWur8up"
      },
      "outputs": [],
      "source": [
        "#dbow\n",
        "model_dbow = Doc2Vec(tagged_data,vector_size=20, min_count=1, epochs=2,dm=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m83v00QIr_os"
      },
      "outputs": [],
      "source": [
        "print(model_dbow.infer_vector(['man','eats','food']))#feature vector of man eats food"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-tYrftYsBkS"
      },
      "outputs": [],
      "source": [
        "model_dbow.wv.most_similar(\"man\",topn=5)#top 5 most simlar words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__n87XnEsERG"
      },
      "outputs": [],
      "source": [
        " model_dbow.wv.n_similarity([\"dog\"],[\"man\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNcctOcisGv3"
      },
      "outputs": [],
      "source": [
        "#dm\n",
        "model_dm = Doc2Vec(tagged_data, min_count=1, vector_size=20, epochs=2,dm=1)\n",
        "\n",
        "print(\"Inference Vector of man eats food\\n \",model_dm.infer_vector(['man','eats','food']))\n",
        "\n",
        "print(\"Most similar words to man in our corpus\\n\",model_dm.wv.most_similar(\"man\",topn=5))\n",
        "print(\"Similarity between man and dog: \",model_dm.wv.n_similarity([\"dog\"],[\"man\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4mUyl4JtZot"
      },
      "source": [
        "#9) Visualizing Embeddings Using TSNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAAjmEZMtlco"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec, KeyedVectors #To load the model\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore') #ignore any generated warnings\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt #to generate the t-SNE plot\n",
        "from sklearn.manifold import TSNE #scikit learn's TSNE\n",
        "\n",
        "import os\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmE1j1yeulsb"
      },
      "outputs": [],
      "source": [
        "# load model\n",
        "try:\n",
        "    import google.colab\n",
        "    model = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/NLP/word2vec_cbow.bin',binary=True)\n",
        "except ModuleNotFoundError:\n",
        "    cwd=os.getcwd()\n",
        "    model = KeyedVectors.load_word2vec_format(cwd+'/content/drive/MyDrive/NLP/word2vec_cbow.bin', binary=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCFsRMvdvGXC"
      },
      "source": [
        "##TSNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfV0sQbhu23d"
      },
      "outputs": [],
      "source": [
        "#Preprocessing our models vocabulary to make better visualizations\n",
        "\n",
        "words_vocab= list(model.wv.vocab)#all the words in the vocabulary.\n",
        "print(\"Size of Vocabulary:\",len(words_vocab))\n",
        "print(\"Few words in Vocabulary\",words_vocab[:50])\n",
        "\n",
        "#Let us remove the stop words from this it will help making the visualization cleaner\n",
        "stopwords_en = stopwords.words()\n",
        "words_vocab_without_sw = [word.lower() for word in words_vocab if not word in stopwords_en]\n",
        "print(\"Size of Vocabulary without stopwords:\",len(words_vocab_without_sw))\n",
        "print(\"Few words in Vocabulary without stopwords\",words_vocab_without_sw[:30])\n",
        "#The size didnt reduce much after removing the stop words so lets try visualizing only a selected subset of words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laOjNoe4u59S"
      },
      "outputs": [],
      "source": [
        "#With the increase in the amount of data, it becomes more and more difficult to visualize and interpret\n",
        "#In practice, similar words are combined into groups for further visualization.\n",
        "\n",
        "keys = ['school', 'year', 'college', 'city', 'states', 'university', 'team', 'film']\n",
        "embedding_clusters = []\n",
        "word_clusters = []\n",
        "\n",
        "for word in keys:\n",
        "    embeddings = []\n",
        "    words = []\n",
        "    for similar_word, _ in model.most_similar(word, topn=30):\n",
        "        words.append(similar_word)\n",
        "        embeddings.append(model[similar_word])\n",
        "    embedding_clusters.append(embeddings)#apending access vector of all similar words\n",
        "    word_clusters.append(words)#appending list of all smiliar words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjVylZcVu8eM"
      },
      "outputs": [],
      "source": [
        "print(\"Embedding clusters:\",embedding_clusters[0][0])#Access vector of the first word only\n",
        "print(\"Word Clousters:\",word_clusters[:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6W4IrWequ_6f"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "\n",
        "embedding_clusters = np.array(embedding_clusters)\n",
        "n, m, k = embedding_clusters.shape #geting the dimensions\n",
        "tsne_model_en_2d = TSNE(perplexity=5, n_components=2, init='pca', n_iter=1500, random_state=2020)\n",
        "embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XdYKPenvRF7"
      },
      "source": [
        "##Hyperparameters of TSNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sbQrFJrvTDo"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "\n",
        "#script for constructing two-dimensional graphics using Matplotlib\n",
        "def tsne_plot_similar_words(labels, embedding_clusters, word_clusters, a=0.7):\n",
        "    plt.figure(figsize=(16, 9))\n",
        "\n",
        "\n",
        "    for label, embeddings, words in zip(labels, embedding_clusters, word_clusters):\n",
        "        x = embeddings[:,0]\n",
        "        y = embeddings[:,1]\n",
        "        plt.scatter(x, y, alpha=a, label=label)\n",
        "        for i, word in enumerate(words):\n",
        "            plt.annotate(word, alpha=0.5, xy=(x[i], y[i]), xytext=(5, 2),\n",
        "                         textcoords='offset points', ha='right', va='bottom', size=8)\n",
        "    plt.legend(loc=4)\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "tsne_plot_similar_words(words_vocab_without_sw, embeddings_en_2d, word_clusters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UkIPee_vZQA"
      },
      "outputs": [],
      "source": [
        "tsne_model_en_2d = TSNE(perplexity=25, n_components=2, init='pca', n_iter=1500, random_state=2020)\n",
        "embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)\n",
        "tsne_plot_similar_words(words_vocab_without_sw, embeddings_en_2d, word_clusters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wp8apZpUvc2k"
      },
      "outputs": [],
      "source": [
        "tsne_model_en_2d = TSNE(perplexity=5, n_components=2, init='pca', n_iter=1500, random_state=2020)\n",
        "embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)\n",
        "tsne_plot_similar_words(words_vocab_without_sw, embeddings_en_2d, word_clusters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10wCq6ssvny9"
      },
      "source": [
        "#10) Visualizing Embeddings using Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-4YFOmBv36-"
      },
      "outputs": [],
      "source": [
        "#making the required imports\n",
        "import warnings #ignoring the generated warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import tensorflow as tf\n",
        "#from tensorflow.contrib import seq2seq\n",
        "#from tensorflow.contrib.rnn import DropoutWrapper\n",
        "#tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "import numpy as np\n",
        "from gensim.models import KeyedVectors\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRSa00etwBRX"
      },
      "outputs": [],
      "source": [
        "#Loading the model\n",
        "cwd=os.getcwd()\n",
        "model = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/NLP/word2vec_cbow.bin',binary=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9Hd9fGCwD-7"
      },
      "outputs": [],
      "source": [
        "#get the model's vocabulary size\n",
        "max_size = len(model.wv.vocab)-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HQA0iLUwH0G"
      },
      "outputs": [],
      "source": [
        "#make a numpy array of 0s with the size of the vocabulary and dimensions of our model\n",
        "w2v = np.zeros((max_size,model.wv.vector_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRWRjNbPwKH9"
      },
      "outputs": [],
      "source": [
        "#Now we create a new file called metadata.tsv where we save all the words in our model\n",
        "#we also store the embedding of each word in the w2v matrix\n",
        "if not os.path.exists('projections'):\n",
        "    os.makedirs('projections')\n",
        "\n",
        "with open(\"projections/metadata.tsv\", 'w+',encoding=\"utf-8\") as file_metadata: #changed    added encoding=\"utf-8\"\n",
        "\n",
        "    for i, word in enumerate(model.wv.index2word[:max_size]):\n",
        "\n",
        "        #store the embeddings of the word\n",
        "        w2v[i] = model.wv[word]\n",
        "\n",
        "        #write the word to a file\n",
        "        file_metadata.write(word + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCnqag9UwOrv"
      },
      "outputs": [],
      "source": [
        "#initializing tf session\n",
        "sess = tf.compat.v1.InteractiveSession()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNn5fEaswQvU"
      },
      "outputs": [],
      "source": [
        "#Initialize the tensorflow variable called embeddings that holds the word embeddings:\n",
        "with tf.device(\"/cpu:0\"):\n",
        "    embedding = tf.Variable(w2v, trainable=False, name='embedding')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oswxEDabwSxA"
      },
      "outputs": [],
      "source": [
        "#Initialize all variables\n",
        "tf.compat.v1.global_variables_initializer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBuABMtMwVD8"
      },
      "outputs": [],
      "source": [
        "#object of the saver class which is actually used for saving and restoring variables to and from our checkpoints\n",
        "saver = tf.train.Checkpoint()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUsb0padWL7G"
      },
      "source": [
        "#Word Embedding using Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9F4GzjfvrR6G"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# here we have to pass url and path\n",
        "# (where you want to save your text file)\n",
        "urllib.request.urlretrieve(\"https://medium.datadriveninvestor.com/convolutional-neural-networks-explained-7fafea4de9c9\",\n",
        "                           \"Data0.txt\")\n",
        "\n",
        "file = open(\"Data0.txt\", \"r\")\n",
        "contents = file.read()\n",
        "soup = BeautifulSoup(contents, 'html.parser')\n",
        "\n",
        "f = open(\"Data.txt\", \"w\")\n",
        "\n",
        "# traverse paragraphs from soup\n",
        "for data in soup.find_all(\"p\"):\n",
        "    sum = data.get_text()\n",
        "    f.writelines(sum)\n",
        "\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBi-KjbQWrvx"
      },
      "outputs": [],
      "source": [
        "# importing all necessary modules\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(action = 'ignore')\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enpJHYSpo3gS"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UyO81i5rj3L"
      },
      "outputs": [],
      "source": [
        "import codecs\n",
        "sample = codecs.open('/content/Data.txt', encoding='utf8')\n",
        "s = sample.read()\n",
        "\n",
        "# Replaces escape character with space\n",
        "f = s.replace(\"\\n\", \" \")\n",
        "\n",
        "data = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cn6ojN8sro4w"
      },
      "outputs": [],
      "source": [
        "# iterate through each sentence in the file\n",
        "for i in sent_tokenize(f):\n",
        "\ttemp = []\n",
        "\n",
        "\t# tokenize the sentence into words\n",
        "\tfor j in word_tokenize(i):\n",
        "\t\ttemp.append(j.lower())\n",
        "\n",
        "\tdata.append(temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6UpIVd8rt_7"
      },
      "outputs": [],
      "source": [
        "# Create CBOW model\n",
        "model1 = gensim.models.Word2Vec(data, min_count = 1,\n",
        "\t\t\t\t\t\t\tvector_size = 100, window = 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvBkn4Dzr07X"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Print results\n",
        "print(\"Similarity between 'operations' \" +\n",
        "\t\t\t\"and 'specific' - CBOW : \",\n",
        "\tmodel1.wv.similarity('operations', 'specific'))\n",
        "\n",
        "print(\"Similarity between 'image' \" +\n",
        "\t\t\t\t\"and 'filter' - CBOW : \",\n",
        "\tmodel1.wv.similarity('image', 'filter'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBFX-bBWr4uW"
      },
      "outputs": [],
      "source": [
        "# Create Skip Gram model\n",
        "model2 = gensim.models.Word2Vec(data, min_count = 1, vector_size = 100,\n",
        "\t\t\t\t\t\t\t\t\t\t\twindow = 5, sg = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iw7CkUq8r8yH"
      },
      "outputs": [],
      "source": [
        "# Print results\n",
        "print(\"Similarity between 'operations' \" +\n",
        "\t\t\"and 'specific' - Skip Gram : \",\n",
        "\tmodel2.wv.similarity('operations', 'specific'))\n",
        "\n",
        "print(\"Similarity between 'image' \" +\n",
        "\t\t\t\"and 'filter' - Skip Gram : \",\n",
        "\tmodel2.wv.similarity('image', 'filter'))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}