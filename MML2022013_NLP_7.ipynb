{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Name Bhavesh Kumar Bohara (MML2022013)\n",
        "\n",
        "Use the link below to implement the code to perform text classification on Imdb dataset using CNN, RNN and LSTM. ( Implement the complete notebook)\n",
        "\n",
        "https://github.com/practical-nlp/practical-nlp-code/blob/master/Ch4/05_DeepNN_Example.ipynb\n",
        "\n",
        "\n",
        "As part of documentation, write your understanding of the code in detail. The book which explains this code is also attached. (Plese refer to chapter 4)"
      ],
      "metadata": {
        "id": "RPxMyu0hhBB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installs three Python packages using pip, a package installer for Python."
      ],
      "metadata": {
        "id": "OEv7YxqJuLdM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMFFWak5g5QR",
        "outputId": "69e7eeda-666d-4d85-b34d-de1a21551f0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numpy==1.19.5\n",
            "  Downloading numpy-1.19.5-cp39-cp39-manylinux2010_x86_64.whl (14.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray 2022.12.0 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
            "xarray-einstats 0.5.1 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
            "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.19.5 which is incompatible.\n",
            "ml-dtypes 0.0.4 requires numpy>1.20, but you have numpy 1.19.5 which is incompatible.\n",
            "matplotlib 3.7.1 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
            "librosa 0.10.0.post2 requires numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3, but you have numpy 1.19.5 which is incompatible.\n",
            "jaxlib 0.4.7+cuda11.cudnn86 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\n",
            "jax 0.4.7 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\n",
            "cmdstanpy 1.1.0 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\n",
            "astropy 5.2.1 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
            "arviz 0.15.1 requires numpy>=1.20.0, but you have numpy 1.19.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.19.5\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wget==3.2\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9676 sha256=3fb8c267af361220b3ab08f997ad90d720e14fd23822a87a0e6693e0c05b0086\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/5f/3e/46cc37c5d698415694d83f607f833f83f0149e49b3af9d0f38\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==1.14.0 (from versions: 2.5.0, 2.5.1, 2.5.2, 2.5.3, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0, 2.6.1, 2.6.2, 2.6.3, 2.6.4, 2.6.5, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.7.1, 2.7.2, 2.7.3, 2.7.4, 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==1.14.0\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install numpy==1.19.5\n",
        "!pip install wget==3.2\n",
        "!pip install tensorflow==1.14.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "here we are importing several Python packages and modules that are required to implement a machine learning model using the TensorFlow framework."
      ],
      "metadata": {
        "id": "kK59Dp7dugfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import tarfile\n",
        "import wget\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from zipfile import ZipFile\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.initializers import Constant"
      ],
      "metadata": {
        "id": "m1Hk97hqh3Hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code, it is used to capture the output of the cell containing file download and extraction commands and suppress any output to the console.\n",
        "\n",
        "it downloads and extracts the GloVe word embeddings dataset and the IMDB movie review dataset to a directory named DATAPATH.\n",
        "\n",
        "If the code is not running in Colab, it creates the Data directory and downloads and extracts the GloVe and IMDB datasets to that directory.\n",
        "\n",
        "Finally, the BASE_DIR variable is set to the path of the directory containing the downloaded datasets, which will be used later in the code to load the datasets into memory."
      ],
      "metadata": {
        "id": "3Ulv99pGu57W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "try:\n",
        "\n",
        "    from google.colab import files\n",
        "\n",
        "    !wget -P DATAPATH http://nlp.stanford.edu/data/glove.6B.zip\n",
        "    !unzip DATAPATH/glove.6B.zip -d DATAPATH/glove.6B\n",
        "\n",
        "    !wget -P DATAPATH http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "    !tar -xvf DATAPATH/aclImdb_v1.tar.gz -C DATAPATH\n",
        "\n",
        "    BASE_DIR = 'DATAPATH'\n",
        "\n",
        "except ModuleNotFoundError:\n",
        "\n",
        "    if not os.path.exists('Data/glove.6B'):\n",
        "        os.mkdir('Data/glove.6B')\n",
        "\n",
        "        url='http://nlp.stanford.edu/data/glove.6B.zip'\n",
        "        wget.download(url,'Data')\n",
        "\n",
        "        temp='Data/glove.6B.zip'\n",
        "        file = ZipFile(temp)\n",
        "        file.extractall('Data/glove.6B')\n",
        "        file.close()\n",
        "\n",
        "\n",
        "\n",
        "    if not os.path.exists('Data/aclImdb'):\n",
        "\n",
        "        url='http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
        "        wget.download(url,'Data')\n",
        "\n",
        "        temp='Data/aclImdb_v1.tar.gz'\n",
        "        tar = tarfile.open(temp, \"r:gz\")\n",
        "        tar.extractall('Data')\n",
        "        tar.close()\n",
        "\n",
        "    BASE_DIR = 'Data'"
      ],
      "metadata": {
        "id": "104wRPkakRJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we define three directory paths using the os.path.join() function:"
      ],
      "metadata": {
        "id": "G3WEm6axvc5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\n",
        "TRAIN_DATA_DIR = os.path.join(BASE_DIR, 'aclImdb/train')\n",
        "TEST_DATA_DIR = os.path.join(BASE_DIR, 'aclImdb/test')"
      ],
      "metadata": {
        "id": "4O_fpnqRiRR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These lines define four constants and assign them with specific values:\n",
        "\n"
      ],
      "metadata": {
        "id": "Q4jwCgMRvvh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "MAX_SEQUENCE_LENGTH = 1000\n",
        "MAX_NUM_WORDS = 20000\n",
        "EMBEDDING_DIM = 100\n",
        "VALIDATION_SPLIT = 0.2\n"
      ],
      "metadata": {
        "id": "9UY65agmiT59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Loading and Preprocessing\n",
        "\n",
        "The code defines a function called get_data that loads the text data and corresponding labels from a given directory into the notebook. The function takes a data_dir parameter which specifies the path to the directory containing the text data. The text samples are stored in a list called texts, while the label names are mapped to numeric IDs and stored in a dictionary called labels_index. The label IDs are then stored in a list called labels. The function returns the texts and labels lists.\n",
        "\n",
        "The function is then called twice to load the training and testing data from their respective directories using the TRAIN_DATA_DIR and TEST_DATA_DIR variables. The function also assigns the label mapping dictionary to a variable called labels_index."
      ],
      "metadata": {
        "id": "kt3EBdKsiYG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to load the data from the dataset into the notebook. Will be called twice - for train and test.\n",
        "def get_data(data_dir):\n",
        "    texts = []  # list of text samples\n",
        "    labels_index = {'pos':1, 'neg':0}  # dictionary mapping label name to numeric id\n",
        "    labels = []  # list of label ids\n",
        "    for name in sorted(os.listdir(data_dir)):\n",
        "        path = os.path.join(data_dir, name)\n",
        "        if os.path.isdir(path):\n",
        "            if name=='pos' or name=='neg':\n",
        "                label_id = labels_index[name]\n",
        "                for fname in sorted(os.listdir(path)):\n",
        "                        fpath = os.path.join(path, fname)\n",
        "                        text = open(fpath,encoding='utf8').read()\n",
        "                        texts.append(text)\n",
        "                        labels.append(label_id)\n",
        "    return texts, labels\n",
        "\n",
        "train_texts, train_labels = get_data(TRAIN_DATA_DIR)\n",
        "test_texts, test_labels = get_data(TEST_DATA_DIR)\n",
        "labels_index = {'pos':1, 'neg':0}\n"
      ],
      "metadata": {
        "id": "5A1HI0eniY9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "here code is using Keras Tokenizer to vectorize text samples. MAX_NUM_WORDS is the maximum number of words to keep in the vocabulary, which is set to 20000. The tokenizer is fitted on training data only using fit_on_texts() function. train_sequences and test_sequences are then created using texts_to_sequences() function, which converts text to a vector of word indexes. word_index contains the dictionary of words extracted from the text data with their corresponding indexes. The output will show the number of unique tokens found in the text data."
      ],
      "metadata": {
        "id": "HC_c6bNfwwOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Vectorize these text samples into a 2D integer tensor using Keras Tokenizer\n",
        "#Tokenizer is fit on training data only, and that is used to tokenize both train and test data.\n",
        "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts) #Converting text to a vector of word indexes\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToGB2UQYieI1",
        "outputId": "dc49a84a-7d12-4691-a1fb-ca7151ae8a11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 88582 unique tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "here the  code is converting the tokenized text sequences into a fixed length sequence of 1000 integers with initial padding of 0's, using the pad_sequences() function. It is also converting the labels into a categorical format using the to_categorical() function. The training data is split into training and validation sets using a validation split ratio of 0.2. Finally, the code prints a message indicating that the splitting of the data into training and validation sets is done."
      ],
      "metadata": {
        "id": "itsgV-DqxAv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting this to sequences to be fed into neural network. Max seq. len is 1000 as set earlier\n",
        "#initial padding of 0s, until vector is of size MAX_SEQUENCE_LENGTH\n",
        "trainvalid_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "trainvalid_labels = to_categorical(np.asarray(train_labels))\n",
        "test_labels = to_categorical(np.asarray(test_labels))\n",
        "\n",
        "# split the training data into a training set and a validation set\n",
        "indices = np.arange(trainvalid_data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "trainvalid_data = trainvalid_data[indices]\n",
        "trainvalid_labels = trainvalid_labels[indices]\n",
        "num_validation_samples = int(VALIDATION_SPLIT * trainvalid_data.shape[0])\n",
        "x_train = trainvalid_data[:-num_validation_samples]\n",
        "y_train = trainvalid_labels[:-num_validation_samples]\n",
        "x_val = trainvalid_data[-num_validation_samples:]\n",
        "y_val = trainvalid_labels[-num_validation_samples:]\n",
        "#This is the data we will use for CNN and RNN training\n",
        "print('Splitting the train data into train and valid is done')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gr_sx6HriiXv",
        "outputId": "6c464cb1-990a-415d-96b4-fbad55380b26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Splitting the train data into train and valid is done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code block prepares the embedding matrix for the pre-trained Glove word embeddings. First, the code reads the Glove embeddings from the file and maps each word to its corresponding embedding vector. Then, it prepares the embedding matrix for the Keras Embedding layer by creating a matrix of size (num_words, EMBEDDING_DIM) and fills it with the embeddings from Glove for each word in the word_index. Finally, it creates an Embedding layer in Keras using the embedding matrix and sets it to be non-trainable."
      ],
      "metadata": {
        "id": "aVQhpDfKxaLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Preparing embedding matrix.')\n",
        "\n",
        "# first, build index mapping words in the embeddings set\n",
        "# to their embedding vector\n",
        "embeddings_index = {}\n",
        "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'),encoding='utf8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print('Found %s word vectors in Glove embeddings.' % len(embeddings_index))\n",
        "#print(embeddings_index[\"google\"])\n",
        "\n",
        "# prepare embedding matrix - rows are the words from word_index, columns are the embeddings of that word from glove.\n",
        "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    if i > MAX_NUM_WORDS:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# load these pre-trained word embeddings into an Embedding layer\n",
        "# note that we set trainable = False so as to keep the embeddings fixed\n",
        "embedding_layer = Embedding(num_words,\n",
        "                            EMBEDDING_DIM,\n",
        "                            embeddings_initializer=Constant(embedding_matrix),\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=False)\n",
        "print(\"Preparing of embedding matrix is done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3CMsZjcim6F",
        "outputId": "4dff574f-69d7-4430-af72-de53cf877414"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing embedding matrix.\n",
            "Found 400000 word vectors in Glove embeddings.\n",
            "Preparing of embedding matrix is done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1D CNN Model with pre-trained embedding\n",
        "here we defines a 1D CNN model for sentiment analysis. It starts by preparing an embedding matrix using pre-trained word embeddings from Glove. Then, the CNN model is defined using the embedding layer and several 1D convolutional layers with max pooling. The model is compiled and trained on the training set, with validation set for tuning. Finally, the model is evaluated on the test set to get the accuracy score."
      ],
      "metadata": {
        "id": "y-TOQOdRizcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Define a 1D CNN model.')\n",
        "\n",
        "cnnmodel = Sequential()\n",
        "cnnmodel.add(embedding_layer)\n",
        "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
        "cnnmodel.add(MaxPooling1D(5))\n",
        "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
        "cnnmodel.add(MaxPooling1D(5))\n",
        "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
        "cnnmodel.add(GlobalMaxPooling1D())\n",
        "cnnmodel.add(Dense(128, activation='relu'))\n",
        "cnnmodel.add(Dense(len(labels_index), activation='softmax'))\n",
        "\n",
        "cnnmodel.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "#Train the model. Tune to validation set.\n",
        "cnnmodel.fit(x_train, y_train,\n",
        "          batch_size=128,\n",
        "          epochs=1, validation_data=(x_val, y_val))\n",
        "#Evaluate on test set:\n",
        "score, acc = cnnmodel.evaluate(test_data, test_labels)\n",
        "print('Test accuracy with CNN:', acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AUzqAx3ivPP",
        "outputId": "cde9b15e-32d7-48b7-c595-314aefa88dbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Define a 1D CNN model.\n",
            "157/157 [==============================] - 145s 918ms/step - loss: 0.6679 - acc: 0.6051 - val_loss: 0.5527 - val_acc: 0.7142\n",
            "782/782 [==============================] - 57s 73ms/step - loss: 0.5555 - acc: 0.7163\n",
            "Test accuracy with CNN: 0.7162799835205078\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1D CNN model with training your own embedding\n",
        "defines and trains a CNN model on the fly without using pre-trained embeddings. The model consists of an embedding layer followed by three 1D convolutional layers with 128 filters each, and global max pooling. The model is then compiled and trained on the training data with a batch size of 128 and 1 epoch. Finally, the model is evaluated on the test set and the accuracy is printed."
      ],
      "metadata": {
        "id": "mFx8WM68i8qK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Defining and training a CNN model, training embedding layer on the fly instead of using pre-trained embeddings\")\n",
        "cnnmodel = Sequential()\n",
        "cnnmodel.add(Embedding(MAX_NUM_WORDS, 128))\n",
        "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
        "cnnmodel.add(MaxPooling1D(5))\n",
        "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
        "cnnmodel.add(MaxPooling1D(5))\n",
        "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
        "cnnmodel.add(GlobalMaxPooling1D())\n",
        "cnnmodel.add(Dense(128, activation='relu'))\n",
        "cnnmodel.add(Dense(len(labels_index), activation='softmax'))\n",
        "\n",
        "cnnmodel.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "#Train the model. Tune to validation set.\n",
        "cnnmodel.fit(x_train, y_train,\n",
        "          batch_size=128,\n",
        "          epochs=1, validation_data=(x_val, y_val))\n",
        "#Evaluate on test set:\n",
        "score, acc = cnnmodel.evaluate(test_data, test_labels)\n",
        "print('Test accuracy with CNN:', acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-mD52Z_i5WY",
        "outputId": "17ef8e24-6501-465a-de75-3af2fdd89052"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defining and training a CNN model, training embedding layer on the fly instead of using pre-trained embeddings\n",
            "157/157 [==============================] - 220s 1s/step - loss: 0.6866 - acc: 0.5275 - val_loss: 0.5982 - val_acc: 0.6878\n",
            "782/782 [==============================] - 69s 88ms/step - loss: 0.6004 - acc: 0.6886\n",
            "Test accuracy with CNN: 0.6886000037193298\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###LSTM Model with training your own embedding\n",
        "\n",
        "In this code snippet, an LSTM model is defined and trained with an embedding layer that is also trained on the fly. The model has a single LSTM layer with 128 units and dropout regularization. The output layer has two units and uses the sigmoid activation function. The model is compiled with the binary cross-entropy loss function, the Adam optimizer, and accuracy metrics. The model is trained for one epoch on the training data with a batch size of 32 and validated on the validation data. Finally, the model is evaluated on the test set and the test accuracy is printed."
      ],
      "metadata": {
        "id": "5z4NBcxSjF_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Defining and training an LSTM model, training embedding layer on the fly\")\n",
        "\n",
        "#model\n",
        "rnnmodel = Sequential()\n",
        "rnnmodel.add(Embedding(MAX_NUM_WORDS, 128))\n",
        "rnnmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "rnnmodel.add(Dense(2, activation='sigmoid'))\n",
        "rnnmodel.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "print('Training the RNN')\n",
        "\n",
        "rnnmodel.fit(x_train, y_train,\n",
        "          batch_size=32,\n",
        "          epochs=1,\n",
        "          validation_data=(x_val, y_val))\n",
        "score, acc = rnnmodel.evaluate(test_data, test_labels,\n",
        "                            batch_size=32)\n",
        "print('Test accuracy with RNN:', acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPYcCihTjG1c",
        "outputId": "8d33f34b-6a45-4b94-bf56-ed49c9077a20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defining and training an LSTM model, training embedding layer on the fly\n",
            "Training the RNN\n",
            "625/625 [==============================] - 1163s 2s/step - loss: 0.4612 - accuracy: 0.7823 - val_loss: 0.3394 - val_accuracy: 0.8608\n",
            "782/782 [==============================] - 186s 238ms/step - loss: 0.3543 - accuracy: 0.8524\n",
            "Test accuracy with RNN: 0.8523600101470947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###LSTM Model using pre-trained Embedding Layer\n",
        "here code snippet, an LSTM model is defined and trained using a pre-trained embedding layer. The rnnmodel2 is created with the Sequential class and the pre-trained embedding layer is added as the first layer. The LSTM layer is added next with a dropout rate of 0.2 to prevent overfitting. The output layer is a dense layer with sigmoid activation function. The model is then compiled with binary cross-entropy loss and adam optimizer.\n",
        "\n",
        "The fit() method is then used to train the model on the training set and validate on the validation set. The evaluate() method is used to evaluate the performance of the trained model on the test set. The accuracy of the model on the test set is printed at the end."
      ],
      "metadata": {
        "id": "8BKGbuTgjPdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Defining and training an LSTM model, using pre-trained embedding layer\")\n",
        "\n",
        "rnnmodel2 = Sequential()\n",
        "rnnmodel2.add(embedding_layer)\n",
        "rnnmodel2.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "rnnmodel2.add(Dense(2, activation='sigmoid'))\n",
        "rnnmodel2.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "print('Training the RNN')\n",
        "\n",
        "rnnmodel2.fit(x_train, y_train,\n",
        "          batch_size=32,\n",
        "          epochs=1,\n",
        "          validation_data=(x_val, y_val))\n",
        "score, acc = rnnmodel2.evaluate(test_data, test_labels,\n",
        "                            batch_size=32)\n",
        "print('Test accuracy with RNN:', acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pxS_JAxjQOL",
        "outputId": "6c8433f2-b367-48b4-83c0-e1bc095c4c46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defining and training an LSTM model, using pre-trained embedding layer\n",
            "Training the RNN\n",
            "625/625 [==============================] - 948s 2s/step - loss: 0.6012 - accuracy: 0.6704 - val_loss: 0.4608 - val_accuracy: 0.7914\n",
            "782/782 [==============================] - 182s 233ms/step - loss: 0.4640 - accuracy: 0.7879\n",
            "Test accuracy with RNN: 0.787880003452301\n"
          ]
        }
      ]
    }
  ]
}